{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Preprocessing\n",
    "[NLTK로 데이터 전처리(Preprocessing) 하기](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=qbxlvnf11&logNo=221434157182)  \n",
    "[딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/21694)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 표제어 추출(Lemmatization)\n",
    "기본 사전형 단어. ie) am, are, is => be  \n",
    "단어의 형태가 적절히 보존됨. 하지만 의미를 알 수 없는 단어가 나오기도 함  \n",
    "표제어 추출기(lemmatizer)가 본래 단어의 품사 정보를 알아야 정확한 결과를 얻을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "n=WordNetLemmatizer()\n",
    "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "# dies => dy, has => ha, watched => watched\n",
    "print([n.lemmatize(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('dies', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('watched', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어간 추출(Stemming)\n",
    "형태학적 분석을 단순화한 버전. 정해진 규칙만 보고 단어의 어미를 자르는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = PorterStemmer()\n",
    "text=\"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "words=word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "text = \"The meaning is well known, even if it is not in complete accord with the reality. The restored stream evokes the environment but is not environmental, evokes history but is not historical, and evokes tradition without being traditional. The reality is conflicted. The restoration was huge in scale and high in cost. The cost overruns alone amounted to $34 million out of a total of about $351 million. Annual maintenance costs have been increasing while the overall number of visitors has declined.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "1. tokenize\n",
    "2. remove stopwords\n",
    "3. remove word less than three letters\n",
    "4. lower\n",
    "5. lemmatization\n",
    "6. stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Data Preprocessing\n",
    "def preprocessing(text):\n",
    "    # 1. tokenize into words\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text)\n",
    "              for word in nltk.word_tokenize(sent)]\n",
    "\n",
    "    print( \"- tokenize into words -\" )\n",
    "    print( tokens )\n",
    "    print()\n",
    "    \n",
    "    # 2. remove stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "\n",
    "    print( \"- remove stopwords -\" )\n",
    "    print( tokens )\n",
    "    print()\n",
    "    \n",
    "    # 3. remove words less than three letters\n",
    "    tokens = [word for word in tokens if len(word) >= 3]\n",
    "\n",
    "    print( \"- remove words less than three letters -\" )\n",
    "    print( tokens )\n",
    "    print()\n",
    "    \n",
    "    # 4. lower capitalization\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    print( \"- lower capitalization -\" )\n",
    "    print( tokens )\n",
    "    print()\n",
    "    \n",
    "    # 5. lemmatization\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    tokens = [lmtzr.lemmatize(word) for word in tokens]\n",
    "\n",
    "    print( \"- lemmatization -\" )\n",
    "    print( tokens )\n",
    "    print()\n",
    "\n",
    "    tokens = [lmtzr.lemmatize(word, 'v') for word in tokens]\n",
    "\n",
    "    print( \"- lemmatization/verb -\" )\n",
    "    print( tokens )\n",
    "    print()\n",
    "\n",
    "    # 6. stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [ stemmer.stem(word) for word in tokens ]\n",
    "\n",
    "    print( \"- stemming -\" )\n",
    "    print(tokens)\n",
    "    print()\n",
    "    \n",
    "    preprocessed_text= ' '.join(tokens)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example : expanding a contraction(영어 줄임말 제거)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbreviation_handler(text):\n",
    "    # case replacement\n",
    "    ln = text\n",
    "    ln = ln.replace(r\"'t\", \" not\")\n",
    "    ln = ln.replace(r\"'s\", \" is\")\n",
    "    ln = ln.replace(r\"'ll\", \" will\")\n",
    "    ln = ln.replace(r\"'ve\", \" have\")\n",
    "    ln = ln.replace(r\"'re\", \" are\")\n",
    "    ln = ln.replace(r\"'m\", \" am\")\n",
    "\n",
    "    # delete single '\n",
    "    ln = ln.replace(r\"'\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example : wiki corpus preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKI_REMOVE_CHARS = re.compile(\"'+|(=+.{2,30}=+)|__TOC__|(ファイル:).+|:(en|de|it|fr|es|kr|zh|no|fi):|\\n\", re.UNICODE)\n",
    "WIKI_SPACE_CHARS = re.compile(\"(\\\\s|゙|゚|　)+\", re.UNICODE)\n",
    "EMAIL_PATTERN = re.compile(\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\", re.UNICODE)\n",
    "URL_PATTERN = re.compile(\"(ftp|http|https)?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", re.UNICODE)\n",
    "WIKI_REMOVE_TOKEN_CHARS = re.compile(\"(\\\\*$|:$|^파일:.+|^;)\", re.UNICODE)\n",
    "MULTIPLE_SPACES = re.compile(' +', re.UNICODE)\n",
    "\n",
    "\n",
    "def tokenize(content, token_min_len=2, token_max_len=100, lower=True):\n",
    "    content = re.sub(EMAIL_PATTERN, ' ', content)  # remove email pattern\n",
    "    content = re.sub(URL_PATTERN, ' ', content) # remove url pattern\n",
    "    content = re.sub(WIKI_REMOVE_CHARS, ' ', content)  # remove unnecessary chars\n",
    "    content = re.sub(WIKI_SPACE_CHARS, ' ', content)\n",
    "    content = re.sub(MULTIPLE_SPACES, ' ', content)\n",
    "    tokens = content.replace(\", )\", \"\").split(\" \")\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        if not token.startswith('_'):\n",
    "            token_candidate = to_unicode(re.sub(WIKI_REMOVE_TOKEN_CHARS, '', token))\n",
    "        else:\n",
    "            token_candidate = \"\"\n",
    "        if len(token_candidate) > 0:\n",
    "            result.append(token_candidate)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example : Corpus preprocessing with remove emoji code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing(text):\n",
    "    # 대괄호 텍스트 제거\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    \n",
    "    # link 제거\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # punctuation 제거\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    \n",
    "    \n",
    "    text = abbreviation_handler(text)\n",
    "\n",
    "    # e-mail 주소 제거\n",
    "    pattern = '(\\[a-zA-Z0-9\\_.+-\\]+@\\[a-zA-Z0-9-\\]+.\\[a-zA-Z0-9-.\\]+)'\n",
    "    text = re.sub(pattern=pattern, repl=' ', string=text)\n",
    "\n",
    "    # url 제거\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:\\da-fA-F]{2}))+'\n",
    "    text = re.sub(pattern=pattern, repl=' ', string=text)\n",
    "\n",
    "    # email 제거\n",
    "    pattern = '(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)'\n",
    "    text = re.sub(pattern=pattern, repl=' ', string=text)\n",
    "\n",
    "    # url 제거(ratsgo github)\n",
    "    pattern = '(ftp|http|https)?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    text = re.sub(pattern=pattern, repl=' ', string=text)\n",
    "\n",
    "    # 한글 자음, 모음 제거\n",
    "    pattern = '([ㄱ-ㅎㅏ-ㅣ])+'\n",
    "    text = re.sub(pattern=pattern, repl=' ', string=text)\n",
    "\n",
    "    # html tag 제거\n",
    "    pattern = '<[^>]*>'\n",
    "    text = re.sub(pattern=pattern, repl=' ', string=text)\n",
    "\n",
    "    # \\r, \\n 제거\n",
    "    pattern = '[\\r|\\n]'\n",
    "    text = re.sub(pattern=pattern, repl=' ', string=text)\n",
    "\n",
    "    # 특수기호 제거\n",
    "    pattern = '[^\\w\\s+.]'\n",
    "    text = re.sub(pattern=pattern, repl=' ', string=text)\n",
    "\n",
    "    # 이모지 제거\n",
    "    pattern = re.compile(\"[\"\n",
    "                         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                         \"]+\", flags=re.UNICODE)\n",
    "    text = pattern.sub(r'', text)\n",
    "\n",
    "    # 이중 space 제거\n",
    "    pattern = re.compile(r'\\s+')\n",
    "    text = re.sub(pattern=pattern, repl=' ', string=text)\n",
    "\n",
    "    return (text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
